{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T06:49:06.356331Z",
     "iopub.status.busy": "2025-01-05T06:49:06.355966Z",
     "iopub.status.idle": "2025-01-05T06:49:11.415385Z",
     "shell.execute_reply": "2025-01-05T06:49:11.414478Z",
     "shell.execute_reply.started": "2025-01-05T06:49:06.356299Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mtcnn\n",
      "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (1.4.2)\n",
      "Collecting lz4>=4.3.3 (from mtcnn)\n",
      "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Downloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lz4, mtcnn\n",
      "Successfully installed lz4-4.3.3 mtcnn-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-05T08:07:22.771Z",
     "iopub.execute_input": "2025-01-05T06:49:11.416777Z",
     "iopub.status.busy": "2025-01-05T06:49:11.416461Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/faces_data_train_fake.json does not exist.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8793c24d824f21957e1ea67f44fbd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/3430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mtcnn import MTCNN\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Initialize paths\n",
    "root_path = '/kaggle/working'\n",
    "data_dir = \"/kaggle/input/deepfake/DFWILD/train_fake/fake\"\n",
    "output_file = \"/kaggle/working/faces_data_train_fake.json\"\n",
    "\n",
    "# Check if the file exists before deleting\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "    print(f\"{output_file} has been deleted successfully.\")\n",
    "else:\n",
    "    print(f\"{output_file} does not exist.\")\n",
    "\n",
    "\n",
    "# Initialize detector\n",
    "detector = MTCNN(device=\"GPU:0\")\n",
    "\n",
    "# Prepare results dictionary and load real images\n",
    "final_results = {}\n",
    "images = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 64  # Adjust batch size based on available resources\n",
    "\n",
    "# Helper function to process a batch\n",
    "def process_batch(image_names):\n",
    "    batch_results = {}\n",
    "    results = detector.detect_faces(image_names)\n",
    "    for image, result in zip(image_names, results):\n",
    "        if len(result) == 0:\n",
    "            continue\n",
    "        coordinates = result[0]\n",
    "        box = result[0]['box']\n",
    "        confidence = result[0]['confidence']\n",
    "        keypoints = result[0]['keypoints']\n",
    "        nose = keypoints['nose']\n",
    "        mouth_right = keypoints['mouth_right']\n",
    "        mouth_left = keypoints['mouth_left']\n",
    "        right_eye = keypoints['right_eye']\n",
    "        left_eye = keypoints['left_eye']\n",
    "        batch_results[image] = {\n",
    "            'box': [int(c) for c in box],\n",
    "            'confidence': float(confidence),\n",
    "            'keypoints': {\n",
    "                'nose': [int(c) for c in nose],\n",
    "                'mouth_right': [int(c) for c in mouth_right],\n",
    "                'mouth_left': [int(c) for c in mouth_left],\n",
    "                'right_eye': [int(c) for c in right_eye],\n",
    "                'left_eye': [int(c) for c in left_eye],\n",
    "            }\n",
    "        }\n",
    "    return batch_results\n",
    "\n",
    "# Process images in batches\n",
    "for i in tqdm(range(0, len(images), batch_size), desc=\"Processing Batches\"):\n",
    "    batch_filenames = images[i:i + batch_size]\n",
    "    batch_results = process_batch(batch_filenames)\n",
    "    final_results.update(batch_results)\n",
    "\n",
    "# Save results to JSON file\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-05T08:07:22.772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mtcnn import MTCNN\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Initialize paths\n",
    "root_path = '/kaggle/working'\n",
    "data_dir = \"/kaggle/input/deepfake/DFWILD/train_real\"\n",
    "output_file = \"/kaggle/working/faces_data_train_real.json\"\n",
    "\n",
    "# Check if the file exists before deleting\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "    print(f\"{output_file} has been deleted successfully.\")\n",
    "else:\n",
    "    print(f\"{output_file} does not exist.\")\n",
    "\n",
    "\n",
    "# Initialize detector\n",
    "detector = MTCNN(device=\"GPU:0\")\n",
    "\n",
    "# Prepare results dictionary and load real images\n",
    "final_results = {}\n",
    "images = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 64  # Adjust batch size based on available resources\n",
    "\n",
    "# Helper function to process a batch\n",
    "def process_batch(image_names):\n",
    "    batch_results = {}\n",
    "    results = detector.detect_faces(image_names)\n",
    "    for image, result in zip(image_names, results):\n",
    "        if len(result) == 0:\n",
    "            continue\n",
    "        coordinates = result[0]\n",
    "        box = result[0]['box']\n",
    "        confidence = result[0]['confidence']\n",
    "        keypoints = result[0]['keypoints']\n",
    "        nose = keypoints['nose']\n",
    "        mouth_right = keypoints['mouth_right']\n",
    "        mouth_left = keypoints['mouth_left']\n",
    "        right_eye = keypoints['right_eye']\n",
    "        left_eye = keypoints['left_eye']\n",
    "        batch_results[image] = {\n",
    "            'box': [int(c) for c in box],\n",
    "            'confidence': float(confidence),\n",
    "            'keypoints': {\n",
    "                'nose': [int(c) for c in nose],\n",
    "                'mouth_right': [int(c) for c in mouth_right],\n",
    "                'mouth_left': [int(c) for c in mouth_left],\n",
    "                'right_eye': [int(c) for c in right_eye],\n",
    "                'left_eye': [int(c) for c in left_eye],\n",
    "            }\n",
    "        }\n",
    "    return batch_results\n",
    "\n",
    "# Process images in batches\n",
    "for i in tqdm(range(0, len(images), batch_size), desc=\"Processing Batches\"):\n",
    "    batch_filenames = images[i:i + batch_size]\n",
    "    batch_results = process_batch(batch_filenames)\n",
    "    final_results.update(batch_results)\n",
    "\n",
    "# Save results to JSON file\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(final_results, f, indent=4)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5797335,
     "sourceId": 9521571,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6408996,
     "sourceId": 10357248,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
