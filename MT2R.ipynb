{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8847660,"sourceType":"datasetVersion","datasetId":5325404},{"sourceId":9521571,"sourceType":"datasetVersion","datasetId":5797335}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.fft\nimport math\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, random_split\nimport pytorch_lightning as pl\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport shutil\nimport random\nfrom torchvision.utils import make_grid\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm\nimport cv2","metadata":{"_uuid":"84d2b032-2baf-45c6-829d-5aa742c0d54f","_cell_guid":"ea046318-769e-4bf0-99ca-83f23660ad9c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.428610Z","iopub.execute_input":"2024-12-25T20:33:54.428891Z","iopub.status.idle":"2024-12-25T20:33:54.433720Z","shell.execute_reply.started":"2024-12-25T20:33:54.428862Z","shell.execute_reply":"2024-12-25T20:33:54.432949Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"batch_size = 16\nepochs = 10\nlearning_rate = 0.001\nweight_decay = 1e-5\nfreeze_backbone = True\nimg_size = 256\ndepth = 4\ndropout = 0.5","metadata":{"_uuid":"a2c970df-ed91-4efd-9795-966f33d292de","_cell_guid":"0dfa88f8-d6e6-4310-8559-acfd8c77f2ce","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.434452Z","iopub.execute_input":"2024-12-25T20:33:54.434792Z","iopub.status.idle":"2024-12-25T20:33:54.446551Z","shell.execute_reply.started":"2024-12-25T20:33:54.434758Z","shell.execute_reply":"2024-12-25T20:33:54.445916Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Define data paths\nroot_path = '/kaggle/working'\ndata_dir = \"/kaggle/input/deepfake/DFWILD\"\ntrain_fake_dir = os.path.join(data_dir, \"train_fake\", \"fake\")\ntrain_real_dir = os.path.join(data_dir, \"train_real\")\ntest_fake_dir = os.path.join(data_dir, \"valid_fake\", \"fake\")\ntest_real_dir = os.path.join(data_dir, \"valid_real\", \"real\")","metadata":{"_uuid":"8cfd469f-ca9f-48f8-a5c1-38407212a678","_cell_guid":"1294556b-517f-4551-a9c5-eb073928bb7b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.447205Z","iopub.execute_input":"2024-12-25T20:33:54.447410Z","iopub.status.idle":"2024-12-25T20:33:54.454504Z","shell.execute_reply.started":"2024-12-25T20:33:54.447382Z","shell.execute_reply":"2024-12-25T20:33:54.453762Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Paths to the working directories\nworking_fake_dir = os.path.join(root_path, \"fake\")\nworking_real_dir = os.path.join(root_path, \"real\")\n\nif not (os.path.exists(working_fake_dir) and os.path.exists(working_real_dir)):\n    # Create the working directories if they don't exist\n    os.makedirs(working_fake_dir, exist_ok=True)\n    os.makedirs(working_real_dir, exist_ok=True)\n\n    # Get the list of all images in train_fake and train_real directories\n    fake_images = [f for f in os.listdir(train_fake_dir) if f.endswith((\".png\", \".jpg\", \".jpeg\"))]\n    real_images = [f for f in os.listdir(train_real_dir) if f.endswith((\".png\", \".jpg\", \".jpeg\"))]\n\n    # Determine the number of real images\n    num_real_images = len(real_images)\n\n    # Randomly sample a subset of fake images equal to the number of real images\n    sampled_fake_images = random.sample(fake_images, num_real_images)\n\n    # Copy the sampled fake images to the working_fake_dir\n    for image in tqdm(sampled_fake_images, desc=\"Copying sampled fake images\"):\n        src_path = os.path.join(train_fake_dir, image)\n        dst_path = os.path.join(working_fake_dir, image)\n        shutil.copy(src_path, dst_path)\n\n    # Copy all real images to the working_real_dir\n    for image in tqdm(real_images, desc=\"Copying real images\"):\n        src_path = os.path.join(train_real_dir, image)\n        dst_path = os.path.join(working_real_dir, image)\n        shutil.copy(src_path, dst_path)\n\ntrain_fake_dir = working_fake_dir\ntrain_real_dir = working_real_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.456578Z","iopub.execute_input":"2024-12-25T20:33:54.456771Z","iopub.status.idle":"2024-12-25T20:33:54.606964Z","shell.execute_reply.started":"2024-12-25T20:33:54.456753Z","shell.execute_reply":"2024-12-25T20:33:54.606254Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=15),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"_uuid":"ffa7c669-01cf-4f31-927c-ac87f6c7c8b3","_cell_guid":"a451ca30-0c25-4fd1-8c75-e04245acde6a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.608114Z","iopub.execute_input":"2024-12-25T20:33:54.608376Z","iopub.status.idle":"2024-12-25T20:33:54.622147Z","shell.execute_reply.started":"2024-12-25T20:33:54.608354Z","shell.execute_reply":"2024-12-25T20:33:54.621530Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Without face cropping","metadata":{}},{"cell_type":"code","source":"# Custom Dataset Loassertader for Real and Fake Images\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, image_dir, label, transform=None):\n        self.image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith(('.png', '.jpg', '.jpeg'))]\n        self.label = label\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, self.label","metadata":{"_uuid":"4b67514e-b5b4-461b-ab37-ba78ea91562c","_cell_guid":"9faded55-9bb5-431b-813f-ef245021ab3c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.622976Z","iopub.execute_input":"2024-12-25T20:33:54.623225Z","iopub.status.idle":"2024-12-25T20:33:54.633736Z","shell.execute_reply.started":"2024-12-25T20:33:54.623182Z","shell.execute_reply":"2024-12-25T20:33:54.632975Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### With face cropping","metadata":{}},{"cell_type":"code","source":"# class CustomDataset(torch.utils.data.Dataset):\n#     def __init__(self, image_dir, label, transform=None, cascade_path=\"haarcascade_frontalface_default.xml\"):\n#         self.image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith(('.png', '.jpg', '.jpeg'))]\n#         self.label = label\n#         self.transform = transform\n#         self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + cascade_path)\n#         self.default_image = Image.new('RGB', (224, 224), color='gray')  # Default placeholder image\n\n#     def __len__(self):\n#         return len(self.image_paths)\n\n#     def __getitem__(self, idx):\n#         image_path = self.image_paths[idx]\n#         # image = Image.open(image_path).convert(\"RGB\")\n#         # if self.transform:\n#         #     image = self.transform(image)\n#         image = cv2.imread(image_path)\n#         if image is None:\n#             print(f\"Error: Unable to read the image file at {image_path}. Returning default image.\")\n#             return self.default_image, self.label\n\n#         # Remove the background and make it black\n#         gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n#         _, mask = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n#         black_background = np.zeros_like(image)\n#         image = cv2.bitwise_and(image, image, mask=mask)\n\n#         # Convert the image to grayscale for face detection\n#         gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n#         # Detect faces in the image\n#         faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n\n#         if len(faces) == 0:\n#             print(f\"No faces detected in the image at {image_path}. Returning default image.\")\n#             if self.transform:\n#                 return self.transform(self.default_image), self.label\n#             return self.default_image, self.label\n\n#         # Assume the largest detected face is the main face\n#         x, y, w, h = max(faces, key=lambda rect: rect[2] * rect[3])\n\n#         # Crop the face from the image\n#         cropped_face = image[y:y+h, x:x+w]\n\n#         # Convert the cropped face to PIL Image\n#         cropped_face = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB)\n#         cropped_face_image = Image.fromarray(cropped_face)\n\n#         # Apply transformations if any\n#         if self.transform:\n#             cropped_face_image = self.transform(cropped_face_image)\n\n#         return cropped_face_image, self.label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.634452Z","iopub.execute_input":"2024-12-25T20:33:54.634693Z","iopub.status.idle":"2024-12-25T20:33:54.647741Z","shell.execute_reply.started":"2024-12-25T20:33:54.634672Z","shell.execute_reply":"2024-12-25T20:33:54.646922Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Load datasets using CustomDataset for both real and fake images\ntrain_real_dataset = CustomDataset(image_dir=train_real_dir, label=1, transform=transform)\ntrain_fake_dataset = CustomDataset(image_dir=train_fake_dir, label=0, transform=transform)\n\ntrain_dataset = torch.utils.data.ConcatDataset([train_fake_dataset, train_real_dataset])\n\n# Split train_dataset into training and validation datasets\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\n# Create test dataset\ntest_real_dataset = CustomDataset(image_dir=test_real_dir, label=1, transform=transform)\ntest_fake_dataset = CustomDataset(image_dir=test_fake_dir, label=0, transform=transform)\n\ntest_dataset = torch.utils.data.ConcatDataset([test_fake_dataset, test_real_dataset])\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"_uuid":"0f6e47ec-cd2a-48d8-8a3f-9e23bb0cb9e2","_cell_guid":"b56fd5ad-d6e1-44cf-9b7a-b4ab887bba44","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.648482Z","iopub.execute_input":"2024-12-25T20:33:54.648682Z","iopub.status.idle":"2024-12-25T20:33:54.796080Z","shell.execute_reply.started":"2024-12-25T20:33:54.648663Z","shell.execute_reply":"2024-12-25T20:33:54.795181Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# def show_images_from_dataloader(dataloader, grids=3, grid_size=3, title=\"Dataset Images\"):\n#     \"\"\"\n#     Display a specified number of grids, each showing a random selection of images from the dataloader.\n\n#     Parameters:\n#     - dataloader (DataLoader): The PyTorch DataLoader to sample images from.\n#     - grids (int): Number of grids to display.\n#     - grid_size (int): Size of the grid (grid_size x grid_size images per grid).\n#     - title (str): Title for the grids.\n#     \"\"\"\n#     for grid in range(grids):\n#         # Sample a batch of images\n#         data_iter = iter(dataloader)\n#         images, labels = next(data_iter)\n\n#         # Select random images for the grid\n#         selected_indices = random.sample(range(len(images)), grid_size * grid_size)\n#         selected_images = [images[idx] for idx in selected_indices]\n\n#         # Create a grid of images\n#         grid_images = make_grid(selected_images, nrow=grid_size, normalize=True, pad_value=1)\n\n#         # Convert to numpy for display\n#         np_grid_images = grid_images.permute(1, 2, 0).cpu().numpy()\n\n#         # Display the grid\n#         plt.figure(figsize=(8, 8))\n#         plt.imshow(np_grid_images)\n#         plt.axis('off')\n#         plt.title(f\"{title} - Grid {grid + 1}\")\n#         plt.show()\n\n# # Display 9 random images in 3 grids for train, validation, and test loaders\n# print(\"Train Loader Grids:\")\n# show_images_from_dataloader(train_loader, grids=3, grid_size=3, title=\"Train Loader\")\n\n# print(\"Validation Loader Grids:\")\n# show_images_from_dataloader(val_loader, grids=3, grid_size=3, title=\"Validation Loader\")\n\n# print(\"Test Loader Grids:\")\n# show_images_from_dataloader(test_loader, grids=3, grid_size=3, title=\"Test Loader\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.796959Z","iopub.execute_input":"2024-12-25T20:33:54.797214Z","iopub.status.idle":"2024-12-25T20:33:54.800877Z","shell.execute_reply.started":"2024-12-25T20:33:54.797171Z","shell.execute_reply":"2024-12-25T20:33:54.800071Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class BaseNetwork(nn.Module):\n    def __init__(self):\n        super(BaseNetwork, self).__init__()\n\n    def print_network(self):\n        if isinstance(self, list):\n            self = self[0]\n        num_params = 0\n        for param in self.parameters():\n            num_params += param.numel()\n        print(\n            'Network [%s] was created. Total number of parameters: %.1f million. '\n            'To see the architecture, do print(network).'\n            % (type(self).__name__, num_params / 1000000)\n        )\n\n    def init_weights(self, init_type='normal', gain=0.02):\n        '''\n        initialize network's weights\n        init_type: normal | xavier | kaiming | orthogonal\n        https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/9451e70673400885567d08a9e97ade2524c700d0/models/networks.py#L39\n        '''\n\n        def init_func(m):\n            classname = m.__class__.__name__\n            if classname.find('InstanceNorm2d') != -1:\n                if hasattr(m, 'weight') and m.weight is not None:\n                    nn.init.constant_(m.weight.data, 1.0)\n                if hasattr(m, 'bias') and m.bias is not None:\n                    nn.init.constant_(m.bias.data, 0.0)\n            elif hasattr(m, 'weight') and (\n                classname.find('Conv') != -1 or classname.find('Linear') != -1\n            ):\n                if init_type == 'normal':\n                    nn.init.normal_(m.weight.data, 0.0, gain)\n                elif init_type == 'xavier':\n                    nn.init.xavier_normal_(m.weight.data, gain=gain)\n                elif init_type == 'xavier_uniform':\n                    nn.init.xavier_uniform_(m.weight.data, gain=1.0)\n                elif init_type == 'kaiming':\n                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n                elif init_type == 'orthogonal':\n                    nn.init.orthogonal_(m.weight.data, gain=gain)\n                elif init_type == 'none':  # uses pytorch's default init method\n                    m.reset_parameters()\n                else:\n                    raise NotImplementedError(\n                        'initialization method [%s] is not implemented'\n                        % init_type\n                    )\n                if hasattr(m, 'bias') and m.bias is not None:\n                    nn.init.constant_(m.bias.data, 0.0)\n\n        self.apply(init_func)\n\n        for m in self.children():\n            if hasattr(m, 'init_weights'):\n                m.init_weights(init_type, gain)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.801667Z","iopub.execute_input":"2024-12-25T20:33:54.801858Z","iopub.status.idle":"2024-12-25T20:33:54.815601Z","shell.execute_reply.started":"2024-12-25T20:33:54.801840Z","shell.execute_reply":"2024-12-25T20:33:54.814933Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class FeedForward2D(nn.Module):\n    def __init__(self, in_channel, out_channel):\n        super(FeedForward2D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                in_channel, out_channel, kernel_size=3, padding=2, dilation=2\n            ),\n            nn.BatchNorm2d(out_channel),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channel),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.816409Z","iopub.execute_input":"2024-12-25T20:33:54.816595Z","iopub.status.idle":"2024-12-25T20:33:54.829970Z","shell.execute_reply.started":"2024-12-25T20:33:54.816579Z","shell.execute_reply":"2024-12-25T20:33:54.829360Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class GlobalFilter(nn.Module):\n    def __init__(self, dim=32, h=80, w=41, fp32fft=True):\n        super().__init__()\n        self.complex_weight = nn.Parameter(\n            torch.randn(h, w, dim, 2, dtype=torch.float32) * 0.02\n        )\n        self.w = w\n        self.h = h\n        self.fp32fft = fp32fft\n\n    def forward(self, x):\n        b, _, a, b = x.size()\n        x = x.permute(0, 2, 3, 1).contiguous()\n\n        if self.fp32fft:\n            dtype = x.dtype\n            x = x.to(torch.float32)\n\n        x = torch.fft.rfft2(x, dim=(1, 2), norm=\"ortho\")\n        weight = torch.view_as_complex(self.complex_weight)\n        x = x * weight\n        x = torch.fft.irfft2(x, s=(a, b), dim=(1, 2), norm=\"ortho\")\n\n        if self.fp32fft:\n            x = x.to(dtype)\n\n        x = x.permute(0, 3, 1, 2).contiguous()\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.830832Z","iopub.execute_input":"2024-12-25T20:33:54.831110Z","iopub.status.idle":"2024-12-25T20:33:54.840838Z","shell.execute_reply.started":"2024-12-25T20:33:54.831082Z","shell.execute_reply":"2024-12-25T20:33:54.840229Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class FreqBlock(nn.Module):\n    def __init__(self, dim, h=80, w=41, fp32fft=True):\n        super().__init__()\n        self.filter = GlobalFilter(dim, h=h, w=w, fp32fft=fp32fft)\n        self.feed_forward = FeedForward2D(in_channel=dim, out_channel=dim)\n\n    def forward(self, x):\n        x = x + self.feed_forward(self.filter(x))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.844027Z","iopub.execute_input":"2024-12-25T20:33:54.844303Z","iopub.status.idle":"2024-12-25T20:33:54.852445Z","shell.execute_reply.started":"2024-12-25T20:33:54.844281Z","shell.execute_reply":"2024-12-25T20:33:54.851659Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def attention(query, key, value):\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(\n        query.size(-1)\n    )\n    p_attn = F.softmax(scores, dim=-1)\n    p_val = torch.matmul(p_attn, value)\n    return p_val, p_attn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.853798Z","iopub.execute_input":"2024-12-25T20:33:54.854000Z","iopub.status.idle":"2024-12-25T20:33:54.863089Z","shell.execute_reply.started":"2024-12-25T20:33:54.853981Z","shell.execute_reply":"2024-12-25T20:33:54.862513Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n    \"\"\"\n    Take in model size and number of heads.\n    \"\"\"\n\n    def __init__(self, patchsize, d_model):\n        super().__init__()\n        self.patchsize = patchsize\n        self.query_embedding = nn.Conv2d(\n            d_model, d_model, kernel_size=1, padding=0\n        )\n        self.value_embedding = nn.Conv2d(\n            d_model, d_model, kernel_size=1, padding=0\n        )\n        self.key_embedding = nn.Conv2d(\n            d_model, d_model, kernel_size=1, padding=0\n        )\n        self.output_linear = nn.Sequential(\n            nn.Conv2d(d_model, d_model, kernel_size=3, padding=1),\n            nn.BatchNorm2d(d_model),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n    def forward(self, x):\n        b, c, h, w = x.size()\n        d_k = c // len(self.patchsize)\n        output = []\n        _query = self.query_embedding(x)\n        _key = self.key_embedding(x)\n        _value = self.value_embedding(x)\n        attentions = []\n        for (width, height), query, key, value in zip(\n            self.patchsize,\n            torch.chunk(_query, len(self.patchsize), dim=1),\n            torch.chunk(_key, len(self.patchsize), dim=1),\n            torch.chunk(_value, len(self.patchsize), dim=1),\n        ):\n            out_w, out_h = w // width, h // height\n\n            # 1) embedding and reshape\n            query = query.view(b, d_k, out_h, height, out_w, width)\n            query = (\n                query.permute(0, 2, 4, 1, 3, 5)\n                .contiguous()\n                .view(b, out_h * out_w, d_k * height * width)\n            )\n            key = key.view(b, d_k, out_h, height, out_w, width)\n            key = (\n                key.permute(0, 2, 4, 1, 3, 5)\n                .contiguous()\n                .view(b, out_h * out_w, d_k * height * width)\n            )\n            value = value.view(b, d_k, out_h, height, out_w, width)\n            value = (\n                value.permute(0, 2, 4, 1, 3, 5)\n                .contiguous()\n                .view(b, out_h * out_w, d_k * height * width)\n            )\n\n            y, _ = attention(query, key, value)\n\n            # 3) \"Concat\" using a view and apply a final linear.\n            y = y.view(b, out_h, out_w, d_k, height, width)\n            y = y.permute(0, 3, 1, 4, 2, 5).contiguous().view(b, d_k, h, w)\n            attentions.append(y)\n            output.append(y)\n\n        output = torch.cat(output, 1)\n        self_attention = self.output_linear(output)\n\n        return self_attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.864081Z","iopub.execute_input":"2024-12-25T20:33:54.864325Z","iopub.status.idle":"2024-12-25T20:33:54.876638Z","shell.execute_reply.started":"2024-12-25T20:33:54.864306Z","shell.execute_reply":"2024-12-25T20:33:54.875828Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    \"\"\"\n    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n    \"\"\"\n\n    def __init__(self, patchsize, in_channel=256):\n        super().__init__()\n        self.attention = MultiHeadedAttention(patchsize, d_model=in_channel)\n        self.feed_forward = FeedForward2D(\n            in_channel=in_channel, out_channel=in_channel\n        )\n\n    def forward(self, rgb):\n        self_attention = self.attention(rgb)\n        output = rgb + self_attention\n        output = output + self.feed_forward(output)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.877360Z","iopub.execute_input":"2024-12-25T20:33:54.877680Z","iopub.status.idle":"2024-12-25T20:33:54.891572Z","shell.execute_reply.started":"2024-12-25T20:33:54.877647Z","shell.execute_reply":"2024-12-25T20:33:54.890781Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class CMA_Block(nn.Module):\n    def __init__(self, in_channel, hidden_channel, out_channel):\n        super(CMA_Block, self).__init__()\n\n        self.conv1 = nn.Conv2d(\n            in_channel, hidden_channel, kernel_size=1, stride=1, padding=0\n        )\n        self.conv2 = nn.Conv2d(\n            in_channel, hidden_channel, kernel_size=1, stride=1, padding=0\n        )\n        self.conv3 = nn.Conv2d(\n            in_channel, hidden_channel, kernel_size=1, stride=1, padding=0\n        )\n\n        self.scale = hidden_channel ** -0.5\n\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(\n                hidden_channel, out_channel, kernel_size=1, stride=1, padding=0\n            ),\n            nn.BatchNorm2d(out_channel),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n    def forward(self, rgb, freq):\n        _, _, h, w = rgb.size()\n\n        q = self.conv1(rgb)\n        k = self.conv2(freq)\n        v = self.conv3(freq)\n\n        q = q.view(q.size(0), q.size(1), q.size(2) * q.size(3)).transpose(\n            -2, -1\n        )\n        k = k.view(k.size(0), k.size(1), k.size(2) * k.size(3))\n\n        attn = torch.matmul(q, k) * self.scale\n        m = attn.softmax(dim=-1)\n\n        v = v.view(v.size(0), v.size(1), v.size(2) * v.size(3)).transpose(\n            -2, -1\n        )\n        z = torch.matmul(m, v)\n        z = z.view(z.size(0), h, w, -1)\n        z = z.permute(0, 3, 1, 2).contiguous()\n\n        output = rgb + self.conv4(z)\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.892268Z","iopub.execute_input":"2024-12-25T20:33:54.892550Z","iopub.status.idle":"2024-12-25T20:33:54.903148Z","shell.execute_reply.started":"2024-12-25T20:33:54.892521Z","shell.execute_reply":"2024-12-25T20:33:54.902356Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class PatchTrans(BaseNetwork):\n    def __init__(self, in_channel, in_size):\n        super(PatchTrans, self).__init__()\n        self.in_size = in_size\n\n        patchsize = [\n            (in_size, in_size),\n            (in_size // 2, in_size // 2),\n            (in_size // 4, in_size // 4),\n            (in_size // 8, in_size // 8),\n        ]\n\n        self.t = TransformerBlock(patchsize, in_channel=in_channel)\n\n    def forward(self, enc_feat):\n        output = self.t(enc_feat)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.903854Z","iopub.execute_input":"2024-12-25T20:33:54.904033Z","iopub.status.idle":"2024-12-25T20:33:54.915777Z","shell.execute_reply.started":"2024-12-25T20:33:54.904016Z","shell.execute_reply":"2024-12-25T20:33:54.915142Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class Classifier2D(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        num_classes,\n        dropout_rate=0.0,\n        act_func=\"softmax\",\n    ):\n        super(Classifier2D, self).__init__()\n        if dropout_rate > 0.0:\n            self.dropout = nn.Dropout(dropout_rate)\n        self.projection = nn.Linear(dim_in, num_classes, bias=True)\n\n        self.act = nn.Sigmoid()\n\n    def forward(self, x):\n        if hasattr(self, \"dropout\"):\n            x = self.dropout(x)\n        x = self.projection(x)\n        x = self.act(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.916604Z","iopub.execute_input":"2024-12-25T20:33:54.916848Z","iopub.status.idle":"2024-12-25T20:33:54.926625Z","shell.execute_reply.started":"2024-12-25T20:33:54.916817Z","shell.execute_reply":"2024-12-25T20:33:54.925938Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class M2TR(BaseNetwork):\n    def __init__(self, img_size, depth, drop_ratio):\n        super(M2TR, self).__init__()\n        img_size = img_size\n        depth = depth\n        drop_ratio = drop_ratio\n        num_classes = 1\n\n        freq_h = img_size // 4\n        freq_w = freq_h // 2 + 1\n\n        self.model = models.efficientnet_b4(pretrained=True)\n\n        texture_dim = 32\n        feature_dim = 1792\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(\n                nn.ModuleList(\n                    [\n                        PatchTrans(in_channel=texture_dim, in_size=freq_h),\n                        FreqBlock(dim=texture_dim, h=freq_h, w=freq_w),\n                        CMA_Block(\n                            in_channel=texture_dim,\n                            hidden_channel=texture_dim,\n                            out_channel=texture_dim,\n                        ),\n                    ]\n                )\n            )\n\n        self.classifier = Classifier2D(\n            feature_dim, num_classes, drop_ratio, \"sigmoid\"\n        )\n\n    def forward(self, x):\n        rgb = x\n        B = rgb.size(0)\n\n        # Set rgb as the output of the second layer of self.model\n        rgb = self.model.features[:3](rgb)\n\n        for attn, filter, cma in self.layers:\n            rgb = attn(rgb)\n            freq = filter(rgb)\n            rgb = cma(rgb, freq)\n\n        # Get the last layer number dynamically\n        last_layer = len(self.model.features)\n\n        # Feed the current rgb value to the 3rd layer of self.model and get the last layer's output\n        for layer_idx in range(3, last_layer):\n            rgb = self.model.features[layer_idx](rgb)\n\n        # Adaptive average pooling and reshaping\n        features = F.adaptive_avg_pool2d(rgb, (1, 1))\n        features = features.view(B, features.size(1))\n\n        # Classification\n        output = self.classifier(features)\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.927356Z","iopub.execute_input":"2024-12-25T20:33:54.927548Z","iopub.status.idle":"2024-12-25T20:33:54.939859Z","shell.execute_reply.started":"2024-12-25T20:33:54.927530Z","shell.execute_reply":"2024-12-25T20:33:54.939044Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class DeepFakeClassifier(nn.Module):\n    def __init__(self, img_size, depth, drop_ratio):\n        super(DeepFakeClassifier, self).__init__()\n        self.classifier = M2TR(img_size, depth, drop_ratio)\n\n    def forward(self, x):\n        x = self.classifier(x)\n        return x","metadata":{"_uuid":"7ee363ad-c6dd-4ace-8703-d98e5da42e70","_cell_guid":"8f677cb4-ef19-448a-877f-35989c6de331","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.940615Z","iopub.execute_input":"2024-12-25T20:33:54.940882Z","iopub.status.idle":"2024-12-25T20:33:54.952362Z","shell.execute_reply.started":"2024-12-25T20:33:54.940853Z","shell.execute_reply":"2024-12-25T20:33:54.951607Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class CustomCriterion:\n    def __init__(self):\n        self.bce_loss = nn.BCELoss()\n\n    def compute_loss(self, outputs, labels):\n        return self.bce_loss(outputs, labels)","metadata":{"_uuid":"6e725796-5568-4f24-bc75-0fc28b05ed0e","_cell_guid":"51466064-91ab-4d9f-b2d4-063f1abbe5c2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.953092Z","iopub.execute_input":"2024-12-25T20:33:54.953343Z","iopub.status.idle":"2024-12-25T20:33:54.966808Z","shell.execute_reply.started":"2024-12-25T20:33:54.953316Z","shell.execute_reply":"2024-12-25T20:33:54.966037Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class DeepFakeClassifierTrainer(pl.LightningModule):\n    def __init__(self, learning_rate, weight_decay, img_size, depth, drop_ratio):\n        super(DeepFakeClassifierTrainer, self).__init__()\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.model = DeepFakeClassifier(img_size, depth, drop_ratio)\n        self.criterion = CustomCriterion()\n        self.training_losses = []\n        self.training_accuracies = []\n        self.validation_losses = []\n        self.validation_accuracies = []\n        self.test_labels = []\n        self.test_outputs = []\n\n    def forward(self, x):\n        return self.model(x)\n\n    def display_metrics(self):\n        plt.figure(figsize=(18, 12))\n\n        plt.subplot(2, 2, 1)\n        plt.plot(range(1, len(self.training_losses) + 1), self.training_losses, label=\"Training Loss\")\n        plt.title(\"Training Loss Over Epochs\")\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n\n        plt.subplot(2, 2, 2)\n        plt.plot(range(1, len(self.validation_losses) + 1), self.validation_losses, label=\"Validation Loss\")\n        plt.title(\"Validation Loss Over Epochs\")\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n\n        plt.subplot(2, 2, 3)\n        plt.plot(range(1, len(self.training_losses) + 1), self.training_accuracies, label=\"Training Accuracy\")\n        plt.title(\"Training Accuracy Over Epochs\")\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n\n        plt.subplot(2, 2, 4)\n        plt.plot(range(1, len(self.validation_losses) + 1), self.validation_accuracies, label=\"Validation Accuracy\")\n        plt.title(\"Validation Accuracy Over Epochs\")\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n\n        plt.tight_layout()\n        plt.show()\n\n    def get_confusion_matrix(self):\n        all_preds = np.concatenate(self.test_outputs)\n        all_labels = np.concatenate(self.test_labels)\n        cm = confusion_matrix(all_labels, all_preds > 0.5)\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Fake\", \"Real\"])\n        disp.plot(cmap=plt.cm.Blues)\n        plt.title(\"Confusion Matrix\")\n        plt.show()\n        return cm\n\n    def training_step(self, batch, batch_idx):\n        inputs, labels = batch\n        inputs = inputs.to(self.device)  # Ensure inputs are on the correct device\n        labels = labels.float().unsqueeze(1).to(self.device)  # Ensure labels are on the correct device\n        outputs = self.model(inputs)\n        loss = self.criterion.compute_loss(outputs, labels)\n        preds = (outputs > 0.5).float()\n        acc = accuracy_score(labels.cpu(), preds.cpu())\n        self.training_losses.append(loss.item())\n        self.training_accuracies.append(acc)\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", acc, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        inputs, labels = batch\n        inputs = inputs.to(self.device)  # Ensure inputs are on the correct device\n        labels = labels.float().unsqueeze(1).to(self.device)  # Ensure labels are on the correct device\n        outputs = self(inputs)\n        loss = self.criterion.compute_loss(outputs, labels)\n        preds = (outputs > 0.5).float()\n        acc = accuracy_score(labels.cpu(), preds.cpu())\n        self.validation_losses.append(loss.item())\n        self.validation_accuracies.append(acc)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        if batch_idx == 0:\n            self.test_labels = []\n            self.test_outputs = []\n\n        inputs, labels = batch\n        inputs = inputs.to(self.device)  # Ensure inputs are on the correct device\n        labels = labels.float().unsqueeze(1).to(self.device)  # Ensure labels are on the correct device\n        outputs = self(inputs)\n        loss = self.criterion.compute_loss(outputs, labels)\n        preds = (outputs > 0.5).float()\n        acc = accuracy_score(labels.cpu(), preds.cpu())\n\n        self.test_labels.append(labels.cpu().numpy())\n        self.test_outputs.append(outputs.cpu().numpy())\n\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", acc, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        return optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)  # Add L2 regularization\n\n    # def on_fit_start(self):\n    #     \"\"\"Ensure compatibility with DDP setup.\"\"\"\n    #     if self.trainer.global_rank == 0:\n    #         self.display_model_summary()\n\n    def display_model_summary(self):\n        print(\"Model Summary:\")\n        print(self.model)\n        self.display_trainable_parameters()\n\n    def display_trainable_parameters(self):\n        total_params = sum(p.numel() for p in self.model.parameters())\n        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n        non_trainable_params = total_params - trainable_params\n        print(\"\\nParameter Summary Table:\")\n        print(f\"{'Parameter Type':<25}{'Count':<15}\")\n        print(f\"{'Trainable Parameters':<25}{trainable_params:<15}\")\n        print(f\"{'Non-Trainable Parameters':<25}{non_trainable_params:<15}\")\n        print(f\"{'Total Parameters':<25}{total_params:<15}\\n\")\n\n    def save_weights(self, path):\n        torch.save(self.model.state_dict(), path)\n\n    def load_weights(self, path):\n        self.model.load_state_dict(torch.load(path))\n\n    def classify_image(self, image_path):\n        self.model.eval()\n        transform = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        image = Image.open(image_path).convert(\"RGB\")\n        image = transform(image).unsqueeze(0).to(self.device)  # Ensure image is on the correct device\n        with torch.no_grad():\n            output = self.model(image)\n            prediction = \"Real\" if output.item() > 0.5 else \"Fake\"\n        print(f\"Prediction: {prediction}, Confidence: {output.item():.4f}\")","metadata":{"_uuid":"715e7cb9-8ed6-48de-92a4-751ed77e712b","_cell_guid":"9b1a8f6f-0a17-46de-9e85-a6c759cef14d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.967621Z","iopub.execute_input":"2024-12-25T20:33:54.967869Z","iopub.status.idle":"2024-12-25T20:33:54.987495Z","shell.execute_reply.started":"2024-12-25T20:33:54.967849Z","shell.execute_reply":"2024-12-25T20:33:54.986674Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Instantiate the model\nmodel = DeepFakeClassifierTrainer(learning_rate, weight_decay, img_size, depth, dropout)","metadata":{"_uuid":"f218f87b-de27-4d9a-8788-021fc7f88c0b","_cell_guid":"d3bfe73e-eb1e-49fb-b3b1-5612c5b0b9c7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-25T20:33:54.988297Z","iopub.execute_input":"2024-12-25T20:33:54.988666Z","iopub.status.idle":"2024-12-25T20:33:55.481777Z","shell.execute_reply.started":"2024-12-25T20:33:54.988635Z","shell.execute_reply":"2024-12-25T20:33:55.481058Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B4_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B4_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Define trainer\ntrainer = pl.Trainer(\n    callbacks=[\n        pl.callbacks.ModelCheckpoint(\n            monitor=\"val_loss\",\n            save_top_k=1,\n            mode=\"min\",\n            dirpath=root_path,\n            filename=\"best-checkpoint\"\n        )\n    ],\n    max_epochs=epochs,\n    accelerator=\"gpu\",\n    devices=2,\n    strategy='ddp_notebook'\n)","metadata":{"_uuid":"d91b5c3e-781d-47e7-a995-7c247915287b","_cell_guid":"7d7f4853-a7fb-49c3-bb33-cbecea5cdbde","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-25T20:33:55.482588Z","iopub.execute_input":"2024-12-25T20:33:55.482878Z","iopub.status.idle":"2024-12-25T20:33:55.560577Z","shell.execute_reply.started":"2024-12-25T20:33:55.482853Z","shell.execute_reply":"2024-12-25T20:33:55.559795Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Path to the last checkpoint\ncheckpoint_path = os.path.join(root_path, \"best-checkpoint.ckpt\")\n\n# Check if the checkpoint exists\nif os.path.exists(checkpoint_path):\n    print(f\"Resuming training from checkpoint: {checkpoint_path}\")\n    trainer.fit(model, train_loader, val_loader, ckpt_path=checkpoint_path)\nelse:\n    print(\"Starting training from scratch.\")\n    trainer.fit(model, train_loader, val_loader)\n","metadata":{"_uuid":"54e72987-8658-4420-b64a-d8e7a0bf039c","_cell_guid":"7c4ded49-da45-4c80-883a-f6ba6ce9121e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-25T20:33:55.561440Z","iopub.execute_input":"2024-12-25T20:33:55.561663Z","iopub.status.idle":"2024-12-25T20:41:46.131528Z","shell.execute_reply.started":"2024-12-25T20:33:55.561640Z","shell.execute_reply":"2024-12-25T20:41:46.129983Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Starting training from scratch.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working exists and is not empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprocess_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Wait for any process to fail or all of them to succeed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         ready = multiprocessing.connection.wait(\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-5ab37487b085>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training from scratch.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SubprocessScriptLauncher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_sigkill_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"],"ename":"NameError","evalue":"name 'exit' is not defined","output_type":"error"}],"execution_count":27},{"cell_type":"code","source":"# trainer.fit(model, train_loader, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:41:46.132252Z","iopub.status.idle":"2024-12-25T20:41:46.132552Z","shell.execute_reply":"2024-12-25T20:41:46.132434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.display_metrics()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:41:46.133143Z","iopub.status.idle":"2024-12-25T20:41:46.133489Z","shell.execute_reply":"2024-12-25T20:41:46.133377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.test(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:41:46.134487Z","iopub.status.idle":"2024-12-25T20:41:46.134809Z","shell.execute_reply":"2024-12-25T20:41:46.134701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.get_confusion_matrix()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:41:46.135739Z","iopub.status.idle":"2024-12-25T20:41:46.136099Z","shell.execute_reply":"2024-12-25T20:41:46.135939Z"}},"outputs":[],"execution_count":null}]}