{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9521571,"sourceType":"datasetVersion","datasetId":5797335},{"sourceId":10383921,"sourceType":"datasetVersion","datasetId":6432587}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile ddp.py\n\nimport warnings\nwarnings.simplefilter(\"ignore\", UserWarning)\n\nimport os\nimport sys\nimport tempfile\nimport torch\nimport json\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\nimport pandas as pd\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, random_split\nfrom PIL import Image, ImageDraw\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, DistributedSampler\n\n# #-------------------------------------------------------------------------------------------\n# Initialize the distributed process\ndef setup_ddp(rank, world_size):\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\n# Cleanup after training\ndef cleanup_ddp():\n    dist.destroy_process_group()\n\ndef cleanup_ddp():\n    dist.destroy_process_group()\n\n# #-------------------------------------------------------------------------------------------\ndef apply_gridmask(image, grid_unit_size_range=(96, 140), keep_ratio=0.6):\n    \"\"\"\n    Applies GridMask to the image.\n    Args:\n        image: A PIL Image to augment.\n        grid_unit_size_range: Tuple (min, max) for the size of the grid units in pixels.\n        keep_ratio: Ratio of the grid unit area to retain.\n    Returns:\n        PIL Image with GridMask applied.\n    \"\"\"\n    width, height = image.size\n\n    # Choose grid unit size randomly within the range\n    grid_unit_size = random.randint(*grid_unit_size_range)\n\n    # Create a binary grid mask\n    mask = np.ones((height, width), dtype=np.uint8)\n\n    d = grid_unit_size\n    l = int(d * keep_ratio)  # Length of retained square\n\n    # Random offsets to shift the grid\n    delta_x = random.randint(0, d - 1)\n    delta_y = random.randint(0, d - 1)\n\n    # Iterate through the grid and apply the mask\n    for y in range(delta_y, height, d):\n        for x in range(delta_x, width, d):\n            mask[y:y + l, x:x + l] = 0\n\n    # Convert mask to 3 channels to match the image\n    mask = np.stack([mask] * 3, axis=-1)\n\n    # Convert PIL Image to NumPy array for masking\n    image_np = np.array(image, dtype=np.uint8)\n    masked_image_np = image_np * mask\n\n    # Convert back to PIL Image\n    return Image.fromarray(masked_image_np)\n\ndef apply_black_patches(image, facial_data, patch_size_range=(30, 50)):\n    \"\"\"\n    Applies black patches on randomly selected facial features.\n    Args:\n        image: A PIL Image to augment.\n        facial_data: Dictionary containing facial feature coordinates.\n        patch_size_range: Tuple (min, max) for the size of the patch.\n    Returns:\n        PIL Image with black patches applied.\n    \"\"\"\n    if not facial_data:\n        return image\n\n    draw = ImageDraw.Draw(image)\n\n    # Randomly select one or more features to blackout\n    features = random.sample(list(facial_data['keypoints'].keys()), random.randint(1, len(facial_data['keypoints'])))\n\n    for feature in features:\n        x, y = facial_data['keypoints'][feature]\n\n        # Determine patch size\n        patch_size = random.randint(*patch_size_range)\n\n        # Calculate patch boundaries\n        left = max(0, x - patch_size // 2)\n        top = max(0, y - patch_size // 2)\n        right = min(image.width, x + patch_size // 2)\n        bottom = min(image.height, y + patch_size // 2)\n\n        # Draw black patch\n        draw.rectangle([left, top, right, bottom], fill=(0, 0, 0))\n\n    return image\n\ndef get_grid_dropout_prob(epoch, max_epochs, initial_prob=0.2, final_prob=0.8):\n    return initial_prob + (final_prob - initial_prob) * (epoch / max_epochs)\n\n# Define Custom Dataset class\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, transform=None, grid_dropout_prob=0.2, face_data_train_fake=None, face_data_train_real=None):\n        \"\"\"\n        Args:\n            dataframe: Pandas dataframe with 'image_path' and 'label' columns.\n            albumentations_transform: Albumentations transformations to be applied.\n            grid_dropout_prob: Probability of applying GridMask or black patches to an image.\n            grid_unit_size_range: Tuple (min, max) for the size of the grid units in pixels.\n            keep_ratio: Ratio of the grid unit area to retain.\n            face_data_train_fake: Dictionary containing facial feature data for fake images.\n            face_data_train_real: Dictionary containing facial feature data for real images.\n        \"\"\"\n        self.dataframe = dataframe\n        self.transform = transform\n        self.grid_dropout_prob = grid_dropout_prob\n        self.face_data_train_fake = face_data_train_fake\n        self.face_data_train_real = face_data_train_real\n\n    def update_grid_dropout_prob(self, new_prob):\n        self.grid_dropout_prob = new_prob\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_path = row['image_path']\n        label = row['label']\n        image = Image.open(image_path).convert(\"RGB\")\n\n        if self.face_data_train_fake:\n            # Select corresponding facial data dictionary based on label\n            facial_data = None\n            if label == 0 and image_path in self.face_data_train_fake:\n                facial_data = self.face_data_train_fake[image_path]\n            elif label == 1 and image_path in self.face_data_train_real:\n                facial_data = self.face_data_train_real[image_path]\n    \n            # Apply augmentation with probability\n            if random.random() < self.grid_dropout_prob:\n                if random.random() < 0.5:\n                    image = apply_gridmask(image)\n                else:\n                    image = apply_black_patches(image, facial_data)\n\n        if self.transform:\n            # Convert PIL Image to NumPy array for Albumentations\n            image = np.array(image)\n    \n            # Apply Albumentations transformations\n            augmented = self.transform(image=image)\n            image = augmented['image']\n\n        return image, label\n\n# #-------------------------------------------------------------------------------------------\nclass FeatureExtractor(nn.Module):\n    def __init__(self, freeze_backbone = True):\n        super(FeatureExtractor, self).__init__()\n        efficientnet = models.efficientnet_b4(pretrained=True)\n        if freeze_backbone:\n            for param in efficientnet.parameters():\n                param.requires_grad = False\n        self.features = efficientnet.features\n\n    def forward(self, x, target_block=None):\n        if target_block == 8:\n            return self.features(x)\n        layers_output = {}\n        for idx, layer in enumerate(self.features):\n            x = layer(x)\n            layers_output[idx] = x\n            # Stop processing if the target block is reached\n            if target_block is not None and idx == target_block:\n                break\n        if target_block is None:\n            target_block = 8\n        return layers_output[target_block]\n\nclass Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        # First Convolutional Block\n        self.conv1 = nn.Conv2d(1792, 1024, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(1024)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout1 = nn.Dropout(0.25)\n\n        # Second Convolutional Block\n        self.conv2 = nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(512)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout2 = nn.Dropout(0.25)\n\n        # Third Convolutional Block\n        self.conv3 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout3 = nn.Dropout(0.25)\n\n        # Fully Connected Layers\n        self.fc1 = nn.Linear(256, 128)\n        self.dropout_fc = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(128, 1)  # Binary classification\n\n    def forward(self, x):\n        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n        x = self.dropout1(x)\n        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n        x = self.dropout2(x)\n        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n        x = self.dropout3(x)\n        # Flatten\n        x = x.view(x.size(0), -1)\n        # Fully Connected Layers\n        x = F.relu(self.fc1(x))\n        x = self.dropout_fc(x)\n        x = torch.sigmoid(self.fc2(x))  # Sigmoid for binary classification\n        return x\n\nclass DeepFakeClassifier(nn.Module):\n    def __init__(self, texture_layer, freeze_backbone):\n        super(DeepFakeClassifier, self).__init__()\n        self.feature_extractor = FeatureExtractor(freeze_backbone)\n        self.classifier = Classifier()\n        self.texture_layer = texture_layer\n\n    def forward(self, x):\n        x = self.feature_extractor(x, self.texture_layer)\n        x = self.classifier(x)\n        return x\n\n# #-------------------------------------------------------------------------------------------\nclass CustomCriterion:\n    def __init__(self):\n        self.bce_loss = nn.BCELoss()\n\n    def compute_loss(self, outputs, labels):\n        return self.bce_loss(outputs, labels)\n\ndef train_one_epoch(model, dataloader, optimizer, criterion, device, epoch):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    for images, labels in tqdm(dataloader, desc=f\"Training Epoch {epoch}\"):\n        images, labels = images.to(device), labels.to(device, dtype=torch.float32)\n\n        optimizer.zero_grad()\n        outputs = model(images).squeeze()\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * images.size(0)\n        preds = (outputs > 0.5).float()\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    return total_loss / total, correct / total\n\n\ndef validate_one_epoch(model, dataloader, criterion, device, epoch):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for images, labels in tqdm(dataloader, desc=f\"Validation Epoch {epoch}\"):\n            images, labels = images.to(device), labels.to(device, dtype=torch.float32)\n\n            outputs = model(images).squeeze()\n            loss = criterion(outputs, labels)\n\n            total_loss += loss.item() * images.size(0)\n            preds = (outputs > 0.5).float()\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    return total_loss / total, correct / total\n\n# #-------------------------------------------------------------------------------------------\ndef create_df_from_dir(directory, label, train_val_test, split):\n    image_paths = [os.path.join(directory, img) for img in os.listdir(directory)]\n    data = {\n        \"image_path\": image_paths,\n        \"label\": [label] * len(image_paths),\n        \"train_val_test\": [train_val_test] * len(image_paths),\n        \"split\": [split] * len(image_paths),\n    }\n    return pd.DataFrame(data)\n\ndef save_to_csv(df, filename):\n    file_path = os.path.join(root_path, filename)\n    df.to_csv(file_path, index=False)\n    print(f\"DataFrame saved to {file_path}\")\n\n# #-------------------------------------------------------------------------------------------\n# Main function for distributed training\ndef main(rank, world_size, train_dataset, val_dataset, split):\n    batch_size = 32\n    epochs = 3\n    texture_layer = 8\n    learning_rate = 0.001\n    weight_decay = 1e-5\n    freeze_backbone = False\n\n    setup_ddp(rank, world_size)\n\n    device = torch.device(f\"cuda:{rank}\")\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n    val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset, num_replicas=world_size, rank=rank)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler)\n\n    model = DeepFakeClassifier(texture_layer, freeze_backbone).to(device)\n\n    prefix = f'model_checkpoint_{split}'\n    model_checkpoints = [f for f in os.listdir('/kaggle/working/') if f.startswith(prefix)]\n    model_checkpoints.sort()\n    if len(model_checkpoints) > 0:\n        checkpoint = model_checkpoints[-1]\n        print(f\"Checkpoint found at {checkpoint}. Loading checkpoint: {rank}...\")\n        model.load_state_dict(torch.load(checkpoint, map_location=device))\n        print(\"Checkpoint loaded successfully.\")\n    else:\n        print(f\"No checkpoint found. Starting training from scratch: {rank}\")\n    \n    model = DDP(model, device_ids=[rank])\n\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n    train_losses = []\n    train_accuracies = []\n    val_losses = []\n    val_accuracies = []\n    \n    for epoch in range(epochs):\n        new_prob = get_grid_dropout_prob(epoch, epochs)\n        if train_dataset.grid_dropout_prob != new_prob:\n            train_dataset.update_grid_dropout_prob(new_prob)\n        train_sampler.set_epoch(epoch)\n        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)\n        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device, epoch)\n\n        if rank == 0:  # Log only from the main process\n            print(f\"Epoch {epoch + 1}/{epochs}:\")\n            print(f\"\\tTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n            print(f\"\\tVal Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n            train_losses.append(train_loss)\n            train_accuracies.append(train_acc)\n            val_losses.append(val_loss)\n            val_accuracies.append(val_acc)\n\n            final_model_path = os.path.join('/kaggle/working/', prefix+f'_epoch_{\"{:02d}\".format(epoch)}_acc_{str(val_acc*10000)[:4]}.pt')\n            torch.save(model.module.state_dict(), final_model_path)\n\n    if rank == 0:  # Save the final best model at the end of training\n        # Plot and save the individual diagrams\n        epochs_range = range(1, epochs + 1)\n\n        # Training Loss\n        plt.figure()\n        plt.plot(epochs_range, train_losses, label='Train Loss')\n        plt.title('Training Loss Over Epochs')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        train_loss_path = os.path.join('/kaggle/working/', f'training_loss_plot-{split}.png')\n        plt.savefig(train_loss_path)\n        print(f\"Training loss diagram saved to {train_loss_path}\")\n\n        # Training Accuracy\n        plt.figure()\n        plt.plot(epochs_range, train_accuracies, label='Train Accuracy')\n        plt.title('Training Accuracy Over Epochs')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        train_accuracy_path = os.path.join('/kaggle/working/', f'training_accuracy_plot-{split}.png')\n        plt.savefig(train_accuracy_path)\n        print(f\"Training accuracy diagram saved to {train_accuracy_path}\")\n\n        # Validation Loss\n        plt.figure()\n        plt.plot(epochs_range, val_losses, label='Validation Loss')\n        plt.title('Validation Loss Over Epochs')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        val_loss_path = os.path.join('/kaggle/working/', f'validation_loss_plot-{split}.png')\n        plt.savefig(val_loss_path)\n        print(f\"Validation loss diagram saved to {val_loss_path}\")\n\n        # Validation Accuracy\n        plt.figure()\n        plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n        plt.title('Validation Accuracy Over Epochs')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        val_accuracy_path = os.path.join('/kaggle/working/', f'validation_accuracy_plot-{split}.png')\n        plt.savefig(val_accuracy_path)\n        print(f\"Validation accuracy diagram saved to {val_accuracy_path}\")\n\n    cleanup_ddp()\n\n# #-------------------------------------------------------------------------------------------\nif __name__ == \"__main__\":\n    argv = sys.argv[1:]\n    world_size = torch.cuda.device_count()\n    print(\"\\nTraining model on split\", argv[0])\n    \n    transform = A.Compose([\n        A.ImageCompression(quality_lower=60, quality_upper=100, p=0.5),\n        A.GaussNoise(var_limit=(10.0, 50.0), p=0.1),\n        A.GaussianBlur(blur_limit=3, p=0.05),\n        A.HorizontalFlip(p=0.5),\n        A.OneOf([\n            A.Resize(256, 256, interpolation=cv2.INTER_AREA),\n            A.Resize(256, 256, interpolation=cv2.INTER_LINEAR),\n        ], p=1),\n        A.PadIfNeeded(min_height=256, min_width=256, border_mode=cv2.BORDER_CONSTANT, value=0),  # Added `value=0`\n        A.OneOf([\n            A.RandomBrightnessContrast(),\n            A.HueSaturationValue(),\n        ], p=0.7),\n        A.ToGray(p=0.1),\n        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, border_mode=cv2.BORDER_CONSTANT, value=0),  # Added `value=0`\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ])\n\n    transform_val = A.Compose([\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ])\n\n    with open('/kaggle/input/face-data/faces_data_train_fake_new.json', 'r') as file:\n        face_data_train_fake = json.load(file)\n    \n    with open('/kaggle/input/face-data/faces_data_train_real_new.json', 'r') as file:\n        face_data_train_real = json.load(file)\n    \n    df_path = '/kaggle/input/face-data/image_paths_dataframe.csv'\n    final_df = pd.read_csv(df_path)\n    train_datasets = []\n    for split in range(1, 6):\n        fake_split_df = final_df[(final_df['train_val_test'] == 'train') & (final_df['split'] == split)]\n        real_split_df = final_df[(final_df['train_val_test'] == 'train') & (final_df['split'] == 0)]\n        combined_df = pd.concat([fake_split_df, real_split_df]).sample(frac=1).reset_index(drop=True)\n        split_dataset = CustomDataset(combined_df, transform=transform, face_data_train_fake=face_data_train_fake, face_data_train_real=face_data_train_real)\n        train_datasets.append(split_dataset)\n    \n    val_df = final_df[(final_df['train_val_test'] == 'val')]\n    val_dataset = CustomDataset(val_df, transform=transform_val)\n\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"3242\"\n\n    torch.multiprocessing.spawn(\n        main,\n        args=(\n            world_size,\n            train_datasets[int(argv[0])],\n            val_dataset,\n            argv[0]\n        ),\n        nprocs=world_size,\n        join=True,\n    )\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-06T11:12:00.066577Z","iopub.execute_input":"2025-01-06T11:12:00.067020Z","iopub.status.idle":"2025-01-06T11:12:00.078823Z","shell.execute_reply.started":"2025-01-06T11:12:00.066961Z","shell.execute_reply":"2025-01-06T11:12:00.077958Z"}},"outputs":[{"name":"stdout","text":"Writing ddp.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!python ddp.py 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T11:12:04.602092Z","iopub.execute_input":"2025-01-06T11:12:04.602412Z","iopub.status.idle":"2025-01-06T12:49:51.001704Z","shell.execute_reply.started":"2025-01-06T11:12:04.602388Z","shell.execute_reply":"2025-01-06T12:49:51.000636Z"}},"outputs":[{"name":"stdout","text":"\nTraining model on split 0\nDownloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\nDownloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n100%|███████████████████████████████████████| 74.5M/74.5M [00:00<00:00, 148MB/s]\n100%|███████████████████████████████████████| 74.5M/74.5M [00:00<00:00, 131MB/s]\nNo checkpoint found. Starting training from scratch: 0\nNo checkpoint found. Starting training from scratch: 1\nTraining Epoch 0: 100%|█████████████████████| 1335/1335 [34:08<00:00,  1.53s/it]\nTraining Epoch 0: 100%|█████████████████████| 1335/1335 [34:08<00:00,  1.53s/it]\nValidation Epoch 0: 100%|███████████████████████| 32/32 [00:13<00:00,  2.45it/s]\nValidation Epoch 0: 100%|███████████████████████| 32/32 [00:13<00:00,  2.36it/s]\nEpoch 1/3:\n\tTrain Loss: 0.3689, Train Acc: 0.8247\n\tVal Loss: 0.1417, Val Acc: 0.9470\nTraining Epoch 1: 100%|█████████████████████| 1335/1335 [31:29<00:00,  1.42s/it]\nTraining Epoch 1: 100%|█████████████████████| 1335/1335 [31:30<00:00,  1.42s/it]\nValidation Epoch 1: 100%|███████████████████████| 32/32 [00:09<00:00,  3.46it/s]\nValidation Epoch 1: 100%|███████████████████████| 32/32 [00:09<00:00,  3.37it/s]\nEpoch 2/3:\n\tTrain Loss: 0.2367, Train Acc: 0.9027\n\tVal Loss: 0.0923, Val Acc: 0.9620\nTraining Epoch 2: 100%|█████████████████████| 1335/1335 [31:01<00:00,  1.39s/it]\nTraining Epoch 2: 100%|█████████████████████| 1335/1335 [31:00<00:00,  1.39s/it]\nValidation Epoch 2: 100%|███████████████████████| 32/32 [00:09<00:00,  3.47it/s]\nEpoch 3/3:\n\tTrain Loss: 0.2172, Train Acc: 0.9105\n\tVal Loss: 0.0696, Val Acc: 0.9690\nValidation Epoch 2: 100%|███████████████████████| 32/32 [00:09<00:00,  3.39it/s]\nTraining loss diagram saved to /kaggle/working/training_loss_plot-0.png\nTraining accuracy diagram saved to /kaggle/working/training_accuracy_plot-0.png\nValidation loss diagram saved to /kaggle/working/validation_loss_plot-0.png\nValidation accuracy diagram saved to /kaggle/working/validation_accuracy_plot-0.png\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}